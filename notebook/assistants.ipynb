{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def QueryResearchUrl(input):\n",
    "    query = input['query']\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    url = list(wikipedia.run(query))\n",
    "    return wikipedia.run(query)\n",
    "\n",
    "# def QueryResearchUrl(input):\n",
    "#     query = input['query']\n",
    "#     wiki = wikipediaapi.Wikipedia(\n",
    "#             user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n",
    "#         )\n",
    "#     page = wiki.page(query)\n",
    "#     if page.exists():\n",
    "#         return page.fullurl\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "def UrlContentScrappingTool(input):\n",
    "    url = input['url']\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    return docs[0].page_content.replace('\\n','')\n",
    "\n",
    "# def UrlContentScrappingTool(input):\n",
    "#     url = input['url']    \n",
    "#     res = requests.get(url)\n",
    "#     soup = bs(res.text, 'html.parser')\n",
    "#     paragraphs = soup.find_all('p')\n",
    "#     content = \"\\n\".join([para.text for para in paragraphs])\n",
    "\n",
    "#     co = json.dumps(content)\n",
    "#     return co\n",
    "\n",
    "\n",
    "def SaveToFileTool(input):\n",
    "    content = input['content']\n",
    "    filename='research2.txt'\n",
    "    with open(f'./files/{filename}', 'w') as f:\n",
    "        f.write(content)\n",
    "    return f\"{filename}에 저장 완료\"\n",
    "\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'QueryResearchUrl',\n",
    "            'description': 'Tool to find Wikipedia URL for a query',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'query': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'What comes in when you invoke the agent'\n",
    "                    }\n",
    "                },\n",
    "                'required': ['query']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'UrlContentScrappingTool',\n",
    "            'description': 'A tool that accesses URLs and scrapes content',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'url': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'URL from QueryResearchUrlTool'\n",
    "                    }\n",
    "                },\n",
    "                'required': ['url']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'SaveToFileTool',\n",
    "            'description': 'Save the content returned with UrlContentScrappingTool as a file.',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'content': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'Content to be saved to the text file'\n",
    "                    }\n",
    "                },\n",
    "                'required': ['content']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# functions = [\n",
    "#     {\n",
    "#         'type': 'function',\n",
    "#         'function': {\n",
    "#             'name': 'QueryResearchUrl',\n",
    "#             'description': 'Tool to find Wikipedia URL for a query',\n",
    "#             'parameters': {\n",
    "#                 'type': 'object',\n",
    "#                 'properties': {\n",
    "#                     'query': {\n",
    "#                         'type': 'string',\n",
    "#                         'description': 'What comes in when you invoke the agent'\n",
    "#                     }\n",
    "#                 },\n",
    "#                 'required': ['query']\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "#     {\n",
    "#         'type': 'function',\n",
    "#         'function': {\n",
    "#             'name': 'UrlContentScrappingTool',\n",
    "#             'description': 'A tool that accesses URLs and scrapes content',\n",
    "#             'parameters': {\n",
    "#                 'type': 'object',\n",
    "#                 'properties': {\n",
    "#                     'url': {\n",
    "#                         'type': 'string',\n",
    "#                         'description': 'URL from QueryResearchUrlTool'\n",
    "#                     }\n",
    "#                 },\n",
    "#                 'required': ['url']\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "#     {\n",
    "#         'type': 'function',\n",
    "#         'function': {\n",
    "#             'name': 'SaveToFileTool',\n",
    "#             'description': 'Save the content returned with UrlContentScrappingTool as a file.',\n",
    "#             'parameters': {\n",
    "#                 'type': 'object',\n",
    "#                 'properties': {\n",
    "#                     'content': {\n",
    "#                         'type': 'string',\n",
    "#                         'description': 'Content to be saved to the text file'\n",
    "#                     }\n",
    "#                 },\n",
    "#                 'required': ['content']\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_clLVGLrllJaf3OzjDtNb6wZU', created_at=1726325066, description=None, instructions='You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.', metadata={}, model='gpt-4o-mini', name='research assistant', object='assistant', tools=[FunctionTool(function=FunctionDefinition(name='QueryResearchUrl', description='Tool to find Wikipedia URL for a query', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'What comes in when you invoke the agent'}}, 'required': ['query']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='UrlContentScrappingTool', description='A tool that accesses URLs and scrapes content', parameters={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'URL from QueryResearchUrlTool'}}, 'required': ['url']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='SaveToFileTool', description='Save the content returned with UrlContentScrappingTool as a file.', parameters={'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to be saved to the text file'}}, 'required': ['content']}, strict=False), type='function')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)\n"
     ]
    }
   ],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"research assistant\",\n",
    "    instructions=\"You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.\",\n",
    "    tools=functions,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_WbDbs64gms09mIiK5j7mj9Kz', created_at=1726327104, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Research about the XZ backdoor.'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(id='msg_lBnyyvy0Ja3GIGFZgeiedFp9', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Research about the XZ backdoor.'), type='text')], created_at=1726327104, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz')]\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "print(list(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_2LIMfpo7iQjdV3RfbExqeQlj', assistant_id='asst_clLVGLrllJaf3OzjDtNb6wZU', cancelled_at=None, completed_at=None, created_at=1726327107, expires_at=1726327707, failed_at=None, incomplete_details=None, instructions='You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o-mini', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz', tool_choice='auto', tools=[FunctionTool(function=FunctionDefinition(name='QueryResearchUrl', description='Tool to find Wikipedia URL for a query', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'What comes in when you invoke the agent'}}, 'required': ['query']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='UrlContentScrappingTool', description='A tool that accesses URLs and scrapes content', parameters={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'URL from QueryResearchUrlTool'}}, 'required': ['url']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='SaveToFileTool', description='Save the content returned with UrlContentScrappingTool as a file.', parameters={'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to be saved to the text file'}}, 'required': ['content']}, strict=False), type='function')], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id\n",
    ")\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_2LIMfpo7iQjdV3RfbExqeQlj', assistant_id='asst_clLVGLrllJaf3OzjDtNb6wZU', cancelled_at=None, completed_at=None, created_at=1726327107, expires_at=1726327707, failed_at=None, incomplete_details=None, instructions='You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o-mini', object='thread.run', parallel_tool_calls=True, required_action=RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_75NmsIUtId4gC0LpSbMbR8Sl', function=Function(arguments='{\"query\":\"XZ backdoor\"}', name='QueryResearchUrl'), type='function')]), type='submit_tool_outputs'), response_format='auto', started_at=1726327108, status='requires_action', thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz', tool_choice='auto', tools=[FunctionTool(function=FunctionDefinition(name='QueryResearchUrl', description='Tool to find Wikipedia URL for a query', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'What comes in when you invoke the agent'}}, 'required': ['query']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='UrlContentScrappingTool', description='A tool that accesses URLs and scrapes content', parameters={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'URL from QueryResearchUrlTool'}}, 'required': ['url']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='SaveToFileTool', description='Save the content returned with UrlContentScrappingTool as a file.', parameters={'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to be saved to the text file'}}, 'required': ['content']}, strict=False), type='function')], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "run_retrieve = client.beta.threads.runs.retrieve(\n",
    "  thread_id=thread.id,\n",
    "  run_id=run.id\n",
    ")\n",
    "print(run_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_2LIMfpo7iQjdV3RfbExqeQlj', assistant_id='asst_clLVGLrllJaf3OzjDtNb6wZU', cancelled_at=None, completed_at=None, created_at=1726327107, expires_at=1726327707, failed_at=None, incomplete_details=None, instructions='You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o-mini', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=1726327108, status='queued', thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz', tool_choice='auto', tools=[FunctionTool(function=FunctionDefinition(name='QueryResearchUrl', description='Tool to find Wikipedia URL for a query', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'What comes in when you invoke the agent'}}, 'required': ['query']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='UrlContentScrappingTool', description='A tool that accesses URLs and scrapes content', parameters={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'URL from QueryResearchUrlTool'}}, 'required': ['url']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='SaveToFileTool', description='Save the content returned with UrlContentScrappingTool as a file.', parameters={'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to be saved to the text file'}}, 'required': ['content']}, strict=False), type='function')], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "new_run = client.beta.threads.runs.submit_tool_outputs(\n",
    "    run_id=run.id,\n",
    "    thread_id=thread.id,\n",
    "    tool_outputs=[\n",
    "        {\n",
    "            'output': QueryResearchUrl({\"query\":\"XZ backdoor\"}),\n",
    "            'tool_call_id': 'call_75NmsIUtId4gC0LpSbMbR8Sl'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(new_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(id='msg_lBnyyvy0Ja3GIGFZgeiedFp9', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Research about the XZ backdoor.'), type='text')], created_at=1726327104, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz')]\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "print(list(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_2LIMfpo7iQjdV3RfbExqeQlj', assistant_id='asst_clLVGLrllJaf3OzjDtNb6wZU', cancelled_at=None, completed_at=None, created_at=1726327107, expires_at=1726327707, failed_at=None, incomplete_details=None, instructions='You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o-mini', object='thread.run', parallel_tool_calls=True, required_action=RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_OEkH7OKq4EumfqjIfexJHnqk', function=Function(arguments='{\"url\": \"https://en.wikipedia.org/wiki/XZ_Utils_backdoor\"}', name='UrlContentScrappingTool'), type='function'), RequiredActionFunctionToolCall(id='call_5fyQTHOqibIZeemxjrgs8ihO', function=Function(arguments='{\"url\": \"https://en.wikipedia.org/wiki/XZ_Utils\"}', name='UrlContentScrappingTool'), type='function'), RequiredActionFunctionToolCall(id='call_E7JnsoH3ot1ZIDTeUcazd1GA', function=Function(arguments='{\"url\": \"https://en.wikipedia.org/wiki/Backdoor_(computing)\"}', name='UrlContentScrappingTool'), type='function')]), type='submit_tool_outputs'), response_format='auto', started_at=1726327147, status='requires_action', thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz', tool_choice='auto', tools=[FunctionTool(function=FunctionDefinition(name='QueryResearchUrl', description='Tool to find Wikipedia URL for a query', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'What comes in when you invoke the agent'}}, 'required': ['query']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='UrlContentScrappingTool', description='A tool that accesses URLs and scrapes content', parameters={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'URL from QueryResearchUrlTool'}}, 'required': ['url']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='SaveToFileTool', description='Save the content returned with UrlContentScrappingTool as a file.', parameters={'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to be saved to the text file'}}, 'required': ['content']}, strict=False), type='function')], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "run_retrieve = client.beta.threads.runs.retrieve(\n",
    "  thread_id=thread.id,\n",
    "  run_id=run.id\n",
    ")\n",
    "print(run_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_2LIMfpo7iQjdV3RfbExqeQlj', assistant_id='asst_clLVGLrllJaf3OzjDtNb6wZU', cancelled_at=None, completed_at=None, created_at=1726327107, expires_at=1726327707, failed_at=None, incomplete_details=None, instructions='You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o-mini', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=1726327147, status='queued', thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz', tool_choice='auto', tools=[FunctionTool(function=FunctionDefinition(name='QueryResearchUrl', description='Tool to find Wikipedia URL for a query', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'What comes in when you invoke the agent'}}, 'required': ['query']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='UrlContentScrappingTool', description='A tool that accesses URLs and scrapes content', parameters={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'URL from QueryResearchUrlTool'}}, 'required': ['url']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='SaveToFileTool', description='Save the content returned with UrlContentScrappingTool as a file.', parameters={'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to be saved to the text file'}}, 'required': ['content']}, strict=False), type='function')], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "required_actions = run_retrieve.required_action.submit_tool_outputs.tool_calls\n",
    "new_run = client.beta.threads.runs.submit_tool_outputs(\n",
    "    run_id=run.id,\n",
    "    thread_id=thread.id,\n",
    "    tool_outputs=[\n",
    "    {\n",
    "        'output': UrlContentScrappingTool(\n",
    "            {\n",
    "                \"url\": json.loads(action.function.arguments)['url']\n",
    "            }),\n",
    "        'tool_call_id': action.id\n",
    "\n",
    "    } for action in required_actions\n",
    "]\n",
    ")\n",
    "print(new_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(id='msg_lBnyyvy0Ja3GIGFZgeiedFp9', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Research about the XZ backdoor.'), type='text')], created_at=1726327104, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz')]\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "print(list(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_2LIMfpo7iQjdV3RfbExqeQlj', assistant_id='asst_clLVGLrllJaf3OzjDtNb6wZU', cancelled_at=None, completed_at=None, created_at=1726327107, expires_at=1726327707, failed_at=None, incomplete_details=None, instructions='You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o-mini', object='thread.run', parallel_tool_calls=True, required_action=RequiredAction(submit_tool_outputs=RequiredActionSubmitToolOutputs(tool_calls=[RequiredActionFunctionToolCall(id='call_2VLeoavaK7Qci1lhlCXxbM7l', function=Function(arguments='{\"content\":\"# XZ Backdoor\\\\n\\\\n## Overview\\\\nIn February 2024, a malicious backdoor was introduced to the Linux utility xz within the liblzma library in versions 5.6.0 and 5.6.1 by an account using the name \\\\\"Jia Tan\\\\\". The backdoor allows an attacker possessing a specific Ed448 private key to execute remote code on the affected Linux system. This issue has been assigned the CVE identifier CVE-2024-3094 and has been given a CVSS score of 10.0, the highest possible score.\\\\n\\\\nAlthough xz is a commonly installed utility in most Linux distributions, the backdoored version had not been widely deployed to production systems at the time of its discovery, but was found in development versions of major distributions. Software developer Andres Freund made the announcement of the backdoor on March 29, 2024.\\\\n\\\\n## Background\\\\nAndres Freund discovered the backdoor after investigating a performance regression in Debian Sid, where he observed high CPU usage and errors in memory debugging tools like Valgrind during SSH connections. His findings were reported to the Openwall Project\\'s open-source security mailing list, drawing attention from various software vendors.\\\\n\\\\nThe attacker, presumed to be Jia Tan, manipulated the project to gain the trust needed to introduce the backdoor. Through a series of sockpuppet accounts, they gradually attained a position of co-maintainer within the XZ Utils project and signed off on the compromised versions of the software.\\\\n\\\\n## Mechanism\\\\nThe malicious code exists in the 5.6.0 and 5.6.1 releases of the XZ Utils software package. The exploit remains dormant unless a specific third-party patch of the SSH server is applied. Under the right conditions, it could allow a malicious actor to bypass authentication and gain remote access to the entire system.\\\\n\\\\nThe backdoor mechanism includes two compressed test files that separate the malicious binary code, which requires extraction and injection into the program. By modifying existing functions in OpenSSH through the shared library loading mechanism, the backdoor gains administrator access when executed properly on a compromised system.\\\\n\\\\n## Response\\\\n### Remediation\\\\nThe U.S. federal Cybersecurity and Infrastructure Security Agency (CISA) issued a security advisory, advising affected devices to revert to a previous version of the software. Prominent Linux vendors have rolled back their versions to safe states, including Red Hat, SUSE, and Debian. Canonical postponed the release of Ubuntu 24.04 LTS and initiated a total binary rebuild of all packages to prevent possible exploitation.\\\\n\\\\n### Broader Response\\\\nThe incident raised concerns about the risks associated with open-source software relying on unpaid volunteers, highlighting the potential for critical utilities to be compromised in unnoticed and damaging ways. Security expert Alex Stamos suggested that had the backdoor gone undetected, it could have led to widespread unauthorized access to sensitive SSH implementations worldwide, posing an enormous security threat.\\\\n\\\\n# Simultaneously Relevant Terms\\\\n## XZ Utils\\\\nXZ Utils is a free software command-line tool for lossless data compression, popular in Unix-like OS and later on Windows. It utilizes the Lempel-Ziv-Markov chain algorithm for compression and decompression. XZ\\'s ability to achieve high compression rates makes it widely used in various Linux distributions for compressing software packages.\\\\n\\\\n## Backdoor (Computing)\\\\nA backdoor refers to a method for bypassing normal security measures, providing unauthorized access to a system. Backdoors can reside in software, hardware, or even firmware and are often used for sustaining covert access to digital environments. They can be deliberately embedded by developers or maliciously inserted by attackers, presenting major security risks.\"}', name='SaveToFileTool'), type='function')]), type='submit_tool_outputs'), response_format='auto', started_at=1726327167, status='requires_action', thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz', tool_choice='auto', tools=[FunctionTool(function=FunctionDefinition(name='QueryResearchUrl', description='Tool to find Wikipedia URL for a query', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'What comes in when you invoke the agent'}}, 'required': ['query']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='UrlContentScrappingTool', description='A tool that accesses URLs and scrapes content', parameters={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'URL from QueryResearchUrlTool'}}, 'required': ['url']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='SaveToFileTool', description='Save the content returned with UrlContentScrappingTool as a file.', parameters={'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to be saved to the text file'}}, 'required': ['content']}, strict=False), type='function')], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "run_retrieve = client.beta.threads.runs.retrieve(\n",
    "  thread_id=thread.id,\n",
    "  run_id=run.id\n",
    ")\n",
    "print(run_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_2LIMfpo7iQjdV3RfbExqeQlj', assistant_id='asst_clLVGLrllJaf3OzjDtNb6wZU', cancelled_at=None, completed_at=None, created_at=1726327107, expires_at=1726327707, failed_at=None, incomplete_details=None, instructions='You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o-mini', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=1726327167, status='queued', thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz', tool_choice='auto', tools=[FunctionTool(function=FunctionDefinition(name='QueryResearchUrl', description='Tool to find Wikipedia URL for a query', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'What comes in when you invoke the agent'}}, 'required': ['query']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='UrlContentScrappingTool', description='A tool that accesses URLs and scrapes content', parameters={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'URL from QueryResearchUrlTool'}}, 'required': ['url']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='SaveToFileTool', description='Save the content returned with UrlContentScrappingTool as a file.', parameters={'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to be saved to the text file'}}, 'required': ['content']}, strict=False), type='function')], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "new_run = client.beta.threads.runs.submit_tool_outputs(\n",
    "    run_id=run.id,\n",
    "    thread_id=thread.id,\n",
    "    tool_outputs=[\n",
    "        {\n",
    "            'output': SaveToFileTool({\"content\":\"\"\"# XZ Backdoor\\\\n\\\\n## Overview\\\\nIn February 2024, a malicious backdoor was introduced to the Linux utility xz within the liblzma library in versions 5.6.0 and 5.6.1 by an account using the name \\\\\"Jia Tan\\\\\". The backdoor allows an attacker possessing a specific Ed448 private key to execute remote code on the affected Linux system. This issue has been assigned the CVE identifier CVE-2024-3094 and has been given a CVSS score of 10.0, the highest possible score.\\\\n\\\\nAlthough xz is a commonly installed utility in most Linux distributions, the backdoored version had not been widely deployed to production systems at the time of its discovery, but was found in development versions of major distributions. Software developer Andres Freund made the announcement of the backdoor on March 29, 2024.\\\\n\\\\n## Background\\\\nAndres Freund discovered the backdoor after investigating a performance regression in Debian Sid, where he observed high CPU usage and errors in memory debugging tools like Valgrind during SSH connections. His findings were reported to the Openwall Project\\'s open-source security mailing list, drawing attention from various software vendors.\\\\n\\\\nThe attacker, presumed to be Jia Tan, manipulated the project to gain the trust needed to introduce the backdoor. Through a series of sockpuppet accounts, they gradually attained a position of co-maintainer within the XZ Utils project and signed off on the compromised versions of the software.\\\\n\\\\n## Mechanism\\\\nThe malicious code exists in the 5.6.0 and 5.6.1 releases of the XZ Utils software package. The exploit remains dormant unless a specific third-party patch of the SSH server is applied. Under the right conditions, it could allow a malicious actor to bypass authentication and gain remote access to the entire system.\\\\n\\\\nThe backdoor mechanism includes two compressed test files that separate the malicious binary code, which requires extraction and injection into the program. By modifying existing functions in OpenSSH through the shared library loading mechanism, the backdoor gains administrator access when executed properly on a compromised system.\\\\n\\\\n## Response\\\\n### Remediation\\\\nThe U.S. federal Cybersecurity and Infrastructure Security Agency (CISA) issued a security advisory, advising affected devices to revert to a previous version of the software. Prominent Linux vendors have rolled back their versions to safe states, including Red Hat, SUSE, and Debian. Canonical postponed the release of Ubuntu 24.04 LTS and initiated a total binary rebuild of all packages to prevent possible exploitation.\\\\n\\\\n### Broader Response\\\\nThe incident raised concerns about the risks associated with open-source software relying on unpaid volunteers, highlighting the potential for critical utilities to be compromised in unnoticed and damaging ways. Security expert Alex Stamos suggested that had the backdoor gone undetected, it could have led to widespread unauthorized access to sensitive SSH implementations worldwide, posing an enormous security threat.\\\\n\\\\n# Simultaneously Relevant Terms\\\\n## XZ Utils\\\\nXZ Utils is a free software command-line tool for lossless data compression, popular in Unix-like OS and later on Windows. It utilizes the Lempel-Ziv-Markov chain algorithm for compression and decompression. XZ\\'s ability to achieve high compression rates makes it widely used in various Linux distributions for compressing software packages.\\\\n\\\\n## Backdoor (Computing)\\\\nA backdoor refers to a method for bypassing normal security measures, providing unauthorized access to a system. Backdoors can reside in software, hardware, or even firmware and are often used for sustaining covert access to digital environments. They can be deliberately embedded by developers or maliciously inserted by attackers, presenting major security risks.\"\"\"}),\n",
    "            'tool_call_id': 'call_2VLeoavaK7Qci1lhlCXxbM7l'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(new_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_2LIMfpo7iQjdV3RfbExqeQlj', assistant_id='asst_clLVGLrllJaf3OzjDtNb6wZU', cancelled_at=None, completed_at=1726327260, created_at=1726327107, expires_at=None, failed_at=None, incomplete_details=None, instructions='You are a capable assistant who helps me find what I want. When you receive a query, find the URL in Wikipedia and scrape the content. After scraping, save it as a text file.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o-mini', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=1726327259, status='completed', thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz', tool_choice='auto', tools=[FunctionTool(function=FunctionDefinition(name='QueryResearchUrl', description='Tool to find Wikipedia URL for a query', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'What comes in when you invoke the agent'}}, 'required': ['query']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='UrlContentScrappingTool', description='A tool that accesses URLs and scrapes content', parameters={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'URL from QueryResearchUrlTool'}}, 'required': ['url']}, strict=False), type='function'), FunctionTool(function=FunctionDefinition(name='SaveToFileTool', description='Save the content returned with UrlContentScrappingTool as a file.', parameters={'type': 'object', 'properties': {'content': {'type': 'string', 'description': 'Content to be saved to the text file'}}, 'required': ['content']}, strict=False), type='function')], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=Usage(completion_tokens=934, prompt_tokens=33928, total_tokens=34862), temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "run_retrieve = client.beta.threads.runs.retrieve(\n",
    "  thread_id=thread.id,\n",
    "  run_id=run.id\n",
    ")\n",
    "print(run_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(id='msg_imKMyQbzSmgdabilLM1WEx5v', assistant_id='asst_clLVGLrllJaf3OzjDtNb6wZU', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='The research about the XZ backdoor has been compiled into a text file. You can access and download it through the link below:\\n\\n[Download XZ Backdoor Research](sandbox:/research2.txt)'), type='text')], created_at=1726327260, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_2LIMfpo7iQjdV3RfbExqeQlj', status=None, thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz'), Message(id='msg_lBnyyvy0Ja3GIGFZgeiedFp9', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Research about the XZ backdoor.'), type='text')], created_at=1726327104, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_WbDbs64gms09mIiK5j7mj9Kz')]\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "print(list(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
